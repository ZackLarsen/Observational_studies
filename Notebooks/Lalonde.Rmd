---
title: "Lalonde"
output: github_document
always_allow_html: true
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

library(here)


source("../R/load_packages.R")

#source(here("R", "load_packages.R"))

```


## Data io

```{r io}

data(lalonde)

```


## Lalonde EDA pt. 1

```{r eda1}

summary(lalonde)

skim(lalonde)

df_status(lalonde)

```


## Lalonde EDA, pt. 2

```{r eda2}


lalonde %>% 
  group_by(treat) %>% 
  count(sort = TRUE, wt = age, name = "subjects")

lalonde %>% 
  group_by(treat) %>% 
  count(decile = 10 * (age %/% 10))


```


## Lalonde EDA, pt. 3

```{r eda3}

summary(lalonde$re78)

describe(lalonde$re78)

hist(lalonde$re78, )

hist(log(lalonde$re78))


ggplot(lalonde, aes(x=age)) + 
  geom_histogram(binwidth=2,colour="white",fill="skyblue") +
  facet_grid(. ~ treat)


```


## Distribution Fitting

```{r fitdistr, eval=TRUE, cache=TRUE, warning=FALSE, message=FALSE}


hist(rgamma(100,2,11))

hist(rnorm(100,5,.01))


x <- rgamma(10000,2,11) + rnorm(10000,5,.01)
fit.gamma <- fitdist(x, distr = "gamma", method = "mle")
summary(fit.gamma)
plot(fit.gamma)


# lalonde_gamma <- fitdist(lalonde$re78 + 1, distr = "gamma", method = "mle")
# summary(lalonde_gamma)
# plot(lalonde_gamma)

```

 ## Matching

#### Nearest neighbor 1:1 match

```{r match_1}


match_1 <- matchit(formula = treat ~ age + educ + black + hispan + married + 
                   nodegree + re74 + re75, 
        method = "nearest",
        data = lalonde,
        discard = "both", 
        replace = FALSE, 
        #caliper = c(),
        #std.caliper = c(),
        ratio = 1)

match_1

summary(match_1, subclass = TRUE, un = FALSE)

```


#### Match diagnostics

```{r diagnostics}


plot(summary(match_1))

love.plot(bal.tab(match_1), stars = "std")

# Examining distributional balance with plots:
bal.plot(match_1, var.name = "nodegree")
bal.plot(match_1, var.name = "distance", mirror = TRUE, type = "histogram")


plot(match_1, type = "jitter", interactive = FALSE)

plot(match_1, type = "qq", interactive = FALSE,
     which.xs = c("age", "married", "re75"))

#eCDF plot
plot(match_1, type = "ecdf", which.xs = c("educ", "married", "re75"))

```


## Matched data

```{r md}

match_1_data <- match.data(match_1)

head(match_1_data)

```


#### Full matching (match 2)

From MatchIt reference:

Given the poor performance of nearest neighbor matching in this example, we can try a different matching method or make other changes to the matching algorithm or distance specification. Below, we’ll try full matching, which matches every treated unit to at least one control and every control to at least one treated unit (Hansen 2004; Stuart and Green 2008). We’ll also try a different link (probit) for the propensity score model.

```{r match_2}


match_2 <- matchit(formula = treat ~ age + educ + black + hispan + married + 
                   nodegree + re74 + re75, 
        method = "full",
        distance = "glm",
        link = "probit",
        data = lalonde,
        discard = "both"
        #replace = FALSE, 
        #caliper = c(),
        #std.caliper = c(),
        #ratio = 1
        )

match_2

summary(match_2, subclass = TRUE, un = FALSE)

```


#### Match diagnostics (match 2)

```{r diagnostics2}

plot(summary(match_2))

love.plot(bal.tab(match_2), stars = "std")

# Examining distributional balance with plots:
bal.plot(match_2, var.name = "nodegree")
bal.plot(match_2, var.name = "distance", mirror = TRUE, type = "histogram")


plot(match_2, type = "jitter", interactive = FALSE)

plot(match_2, type = "qq", interactive = FALSE,
     which.xs = c("age", "married", "re75"))

#eCDF plot
plot(match_2, type = "ecdf", which.xs = c("educ", "married", "re75"))

```


#### Matched data (match 2)

```{r md2}

match_2_data <- match.data(match_2)

```


## Estimating the Treatment Effect

https://kosukeimai.github.io/MatchIt/articles/MatchIt.html

How treatment effects are estimated depends on what form of matching was performed. See vignette("estimating-effects") for information on the variety of way to estimate effects and standard errors after each type of matching and for several outcome types. After 1:1 matching without replacement (i.e., the first matching specification above), we can run a simple regression of the outcome on the treatment in the matched sample (i.e., including the matching weights). With continuous outcomes, it is often a good idea to also include the covariates used in the matching in the effect estimation, as doing so can provide additional robustness to slight imbalances remaining after the matching and can improve precision.

Even though the 1:1 matching was not successful, we’ll demonstrate here how to estimate a treatment effect after performing such an analysis. First, we’ll extract the matched dataset from the matchit object using match.data(). This dataset only contains the matched units and adds columns for distance, weights, and subclass (described previously).

We can then estimate a treatment effect in this dataset using the standard regression functions in R, like lm() or glm(), being sure to include the matching weights (stored in the weights variable of the match.data() output) in the estimation3. We recommend using cluster-robust standard errors for most analyses, with pair membership as the clustering variable; the lmtest and sandwich packages together make this straightforward.

Although there are many possible ways to include covariates (i.e., not just main effects but interactions, smoothing terms like splines, or other nonlinear transformations), it is important not to engage in specification search (i.e., trying many outcomes models in search of the “best” one). Doing so can invalidate results and yield a conclusion that fails to replicate. For this reason, we recommend only including the same terms included in the propensity score model unless there is a strong a priori and justifiable reason to model the outcome differently. Second, it is important not to interpret the coefficients and tests of the other covariates in the outcome model. These are not causal effects and their estimates may be severely confounded. Only the treatment effect estimate can be interpreted as causal assuming the relevant assumptions about unconfoundedness are met. **Inappropriately interpreting the coefficients of covariates in the outcome model is known as the Table 2 fallacy** (Westreich and Greenland 2013). To avoid this, in all examples that incorporate covariates in the outcome model, we restrict the output of outcome regression models to just the treatment coefficient.


#### Linear Modeling

#### Assumptions:
* Linearity
* Homoscedasticity
* Independence
* Normality

http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/

https://data.library.virginia.edu/diagnostic-plots/

```{r assumptions}



```


#### Pre-match data model

```{r pre_match_mod}

plot(lalonde$re78)

# Pre-match data:
prematch_lm <- lm(re78 ~ treat, data = lalonde)

autoplot(prematch_lm) +
  theme_abyss()
  #theme_minimal() + 
  #theme_ft_rc()

```


#### Model 1 sandwich 

```{r sandwich1}

fit1 <- lm(re78 ~ treat + age + educ + black + hispan + married + nodegree + 
             re74 + re75, 
           data = match_1_data, 
           weights = weights)

coeftest(fit1, vcov. = vcovCL, cluster = ~subclass)

```


#### Model 2 sandwich

```{r sandwich2}

fit2 <- lm(re78 ~ treat + age + educ + black + hispan + married + nodegree + 
             re74 + re75, 
           data = match_2_data, 
           weights = weights)

coeftest(fit2, vcov. = vcovCL, cluster = ~subclass)

```

Note that the "treat" coefficient in the linear regression model for the first match is not statistically significant, but it is in the linear regression model for the second match.


## Uncertainty / confidence intervals

#### Match 1 bootstrap

https://kosukeimai.github.io/MatchIt/articles/estimating-effects.html#after-pair-matching-with-replacement-1

```{r uncertainty1}

#Block bootstrap confidence interval
# library(boot)

pair_ids <- levels(match_1_data$subclass)

est_fun <- function(pairs, i) {
  
  #Compute number of times each pair is present
  numreps <- table(pairs[i])
  
  #For each pair p, copy corresponding md row indices numreps[p] times
  ids <- unlist(lapply(pair_ids[pair_ids %in% names(numreps)],
                       function(p) rep(which(match_1_data$subclass == p), 
                                              numreps[p])))
  
  #Subset md with block bootstrapped ids
  md_boot <- match_1_data[ids,]
  
  #Effect estimation
  fit_boot <- lm(re78 ~ treat + age + educ + black + hispan + married + nodegree + 
             re74 + re75,
                 data = md_boot,
                 weights = weights)
  
  #Return the coefficient on treatment
  return(coef(fit_boot)["treat"])
}

boot_est <- boot(pair_ids, est_fun, R = 999)
boot_est


boot.ci(boot_est, type = "bca")

```


#### Match 2 bootstrap

https://moderndive.com/10-inference-for-regression.html is another helpful resource in addition to the above link from Kosuke Imai

```{r uncertainty2}

#Block bootstrap confidence interval
# library(boot)

pair_ids <- levels(match_2_data$subclass)

est_fun <- function(pairs, i) {
  
  #Compute number of times each pair is present
  numreps <- table(pairs[i])
  
  #For each pair p, copy corresponding md row indices numreps[p] times
  ids <- unlist(lapply(pair_ids[pair_ids %in% names(numreps)],
                       function(p) rep(which(match_2_data$subclass == p), 
                                              numreps[p])))
  
  #Subset md with block bootstrapped ids
  md_boot <- match_1_data[ids,]
  
  #Effect estimation
  fit_boot <- lm(re78 ~ treat + age + educ + black + hispan + married + nodegree +
                   re74 + re75,
                 data = md_boot,
                 weights = weights)
  
  #Return the coefficient on treatment
  return(coef(fit_boot)["treat"])
}

boot_est <- boot(pair_ids, est_fun, R = 999)
boot_est


boot.ci(boot_est, type = "bca")

```


## Generalized Linear Modeling
* Get help page:
* ?family

See https://fromthebottomoftheheap.net/2016/06/07/rootograms/

```{r glm}


# Gaussian-distributed residuals and identity link = OLS:
gaussian_identity <- glm(re78 ~ treat + age + educ + black + hispan + married + nodegree + re74 + re75, 
                         data = match_2_data, 
                         family = gaussian(link = "identity"),
                         weights = weights)
summary(gaussian_identity)



# Gamma default link is "inverse"
gamma_inverse <- glm(re78 + 1 ~ treat + age + educ + black + hispan + married + nodegree + re74 + re75, data = match_2_data, family = Gamma(link = "inverse"))
summary(gamma_inverse)




# Using log link
# gamma_log <- glm(re78 + 1 ~ treat + age + educ + black + hispan + married + nodegree + re74 + re75, data = match_2_data, family=Gamma(link = "log"))
# summary(gamma_log)

# # Fixing error from above due to inf values:
# match_2_data_new <- match_2_data # Duplicate data
# match_2_data_new[is.na(match_2_data_new) | match_2_data_new == "Inf"] <- NA  # Replace NaN & Inf with NA
# 
# # Add in positive constant:
# match_2_data_new$re78 <- match_2_data_new$re78 + 1
# 
# gamma_log <- glm(re78 ~ treat + age + educ + black + hispan + married + nodegree + re74 + re75, 
#                  data = match_2_data_new, 
#                  family = Gamma(link = "log"),
#                  weights = weights)
# summary(gamma_log)


# plot(rstudent(gamma_log))
# plot(influence(gamma_log)$hat)
# plot(cooks.distance(gamma_log))

```

#### Zelig

https://docs.zeligproject.org/articles/att.html

All models in Zelig can be estimated and results explored presented using four simple functions:

1. zelig to estimate the parameters,
1. setx to set fitted values for which we want to find quantities of interest,
1. sim to simulate the quantities of interest,
1. plot to plot the simulation results.


```{r zelig}


# https://docs.zeligproject.org/articles/zelig5_vs_zelig4.html
z5 <- zls$new()
z5$zelig(re78 + 1 ~ treat + age + educ + black + hispan + married + nodegree + re74 + re75,
         weights = match_2_data$weights,
         data = match_2_data)
z5$setx()
z5$sim()
z5$graph()



w <- match_2_data$weights
z.out <- zelig(re78 + 1 ~ treat, model = "gamma", weights = w, data = match_2_data)
summary(z.out)
x.out <- setx(z.out)
s.out <- sim(z.out, x = x.out, treat = 1)

s.out
plot(s.out)


x.low <- setx(z.out, treat = 0)
x.high <- setx(z.out, treat = 1)

s.out <- sim(z.out, x = x.low, x1 = x.high)

s.out
plot(s.out)

```


#### Zelig experimentation

```{r zelig_exp}


#Generatng data similar to Austin (2009) for demonstrating treatment effect estimation
gen_X <- function(n) {
  X <- matrix(rnorm(9 * n), nrow = n, ncol = 9)
  X[,5] <- as.numeric(X[,5] < .5)
  X
}

#~20% treated
gen_A <- function(X) {
  LP_A <- - 1.2 + log(2)*X[,1] - log(1.5)*X[,2] + log(2)*X[,4] - log(2.4)*X[,5] + log(2)*X[,7] - log(1.5)*X[,8]
  P_A <- plogis(LP_A)
  rbinom(nrow(X), 1, P_A)
}

# Continuous outcome
gen_Y_C <- function(A, X) {
  2*A + 2*X[,1] + 2*X[,2] + 2*X[,3] + 1*X[,4] + 2*X[,5] + 1*X[,6] + rnorm(length(A), 0, 5)
}
#Conditional:
#  MD: 2
#Marginal:
#  MD: 2

# Binary outcome
gen_Y_B <- function(A, X) {
  LP_B <- -2 + log(2.4)*A + log(2)*X[,1] + log(2)*X[,2] + log(2)*X[,3] + log(1.5)*X[,4] + log(2.4)*X[,5] + log(1.5)*X[,6]
  P_B <- plogis(LP_B)
  rbinom(length(A), 1, P_B)
}
#Conditional:
#  OR:   2.4
#  logOR: .875
#Marginal:
#  RD:    .144
#  RR:   1.54
#  logRR: .433
#  OR:   1.92
#  logOR  .655

# Survival outcome
gen_Y_S <- function(A, X) {
  LP_S <- -2 + log(2.4)*A + log(2)*X[,1] + log(2)*X[,2] + log(2)*X[,3] + log(1.5)*X[,4] + log(2.4)*X[,5] + log(1.5)*X[,6]
  sqrt(-log(runif(length(A)))*2e4*exp(-LP_S))
}
#Conditional:
#  HR:   2.4
#  logHR: .875
#Marginal:
#  HR:   1.57
#  logHR: .452

set.seed(19599)

n <- 2000
X <- gen_X(n)
A <- gen_A(X)

Y_C <- gen_Y_C(A, X)
Y_B <- gen_Y_B(A, X)
Y_S <- gen_Y_S(A, X)

d <- data.frame(A, X, Y_C, Y_B, Y_S)

head(d)





mNN <- matchit(A ~ X1 + X2 + X3 + X4 + X5 + 
                 X6 + X7 + X8 + X9, data = d)

mNN

md <- match.data(mNN)

head(md)


#Linear model without covariates
fit1 <- lm(Y_C ~ A, data = md, weights = weights)
summary(fit1)
#Cluster-robust standard errors
coeftest(fit1, vcov. = vcovCL, cluster = ~subclass)












# data(sanction)
# zqi.out <- zelig(num ~ target + coop + mil, 
#                  model = "poisson", data = sanction, cite = FALSE)
# 
# # find the ATT where the treatement is mil = 1
# z.att <- zqi.out %>%
#              ATT(treatment = "mil", treat = 1) %>% 
#              get_qi(qi = "ATT", xvalue = "TE")
# 
# # summarize the results
# hist(z.att, 
#      main = NULL,
#      xlab ="Average Treatment Effect of mil on the Treated")

s

```


## TWANG IPTW

https://www.rand.org/statistics/twang/r-tutorial.html

```{r twang}

set.seed(1)

ps.lalonde <- ps(treat ~ age + educ + black + hispan + nodegree + 
                   married + re74 + re75,
                 data = lalonde,
                 n.trees=5000,
                 interaction.depth=2,
                 shrinkage=0.01,
                 perm.test.iters=0,
                 stop.method=c("es.mean","ks.max"),
                 estimand = "ATT",
                 verbose=FALSE)

plot(ps.lalonde)

summary(ps.lalonde)


summary(
  ps.lalonde$gbm.obj,
        n.trees=ps.lalonde$desc$ks.max.ATT$n.trees,
        plot=FALSE)


lalonde.balance <- bal.table(ps.lalonde)

lalonde.balance

plot(ps.lalonde, plots=2)

plot(ps.lalonde, plots=3)

plot(ps.lalonde, plots = 4)

plot(ps.lalonde, plots = 5)

plot(ps.lalonde, plots = 3, subset = 2)

# A separate R package, the survey package, is useful for performing the outcomes analyses using weights. Its statistical methods account for the weights when computing standard error estimates. It is not a part of the standard R installation but installing twang should automatically install survey as well.
# 
# > library(survey)
# 
# The get.weights() function extracts the propensity score weights from a ps object. Those weights may then be used as case weights in a svydesign object. By default, it returns weights corresponding to the estimand (ATE or ATT) that was specified in the original call to ps(). If needed, the user can override the default via the optional estimand argument.

lalonde$w <- get.weights(ps.lalonde, stop.method="es.mean")

design.ps <- svydesign(ids=~1, weights=~w, data=lalonde)

glm1 <- svyglm(re78 ~ treat, design=design.ps)

summary(glm1)

```

The analysis estimates an increase in earnings of $733 for those that participated in the

NSW compared with similarly situated people observed in the CPS. The effect, however, does not appear to be statistically significant.


#### TWANG Doubly Robust

Some authors have recommended utilizing both propensity score adjustment and additional covariate adjustment to minimize mean square error or to obtain "doubly robust" estimates of the treatment effect (Huppler-Hullsiek & Louis 2002, Bang & Robins 2005). These estimators are consistent if either the propensity scores are estimated correctly or the regression model is specified correctly. For example, note that the balance table for ks.max.ATT made the two groups more similar on nodegree, but still some differences remained, 70.8% of the treatment group had no degree while 60.1% of the comparison group had no degree. While linear regression is sensitive to model misspecification when the treatment and comparison groups are dissimilar, the propensity score weighting has made them more similar, perhaps enough so that additional modeling with covariates can adjust for any remaining differences. In addition to potential bias reduction, the inclusion of additional covariates can reduce the standard error of the treatment effect if some of the covariates are strongly related to the outcome.

```{r twang2}

glm2 <- svyglm(re78 ~ treat + nodegree, design=design.ps)

summary(glm2)

```

Adjusting for the remaining group difference in the nodegree variable slightly increased the

estimate of the program’s effect to $920, but the difference is still not statistically significant. We can further adjust for the other covariates, but that too in this case has little effect on the estimated program effect.

```{r twang3}

glm3 <- svyglm(re78 ~ treat + age + educ + black + hispan + nodegree +
                 married + re74 + re75,
               design=design.ps)

summary(glm3)

```


#### TWANG LM

The more traditional regression approach to estimating the program effect would fit a linear model with a treatment indicator and linear terms for each of the covariates.

```{r twang4}


glm4 <- lm(re78 ~ treat + age + educ + black + hispan + nodegree +
             married + re74 + re75,
           data=lalonde)

summary(glm4)


```

This model estimates a rather strong treatment effect, estimating a program effect of `$`1548 with a p-value=0.048. Several variations of this regression approach also estimate strong program effects. For example using square root transforms on the earnings variables yields a p- value=0.016. These estimates, however, are very sensitive to the model structure since the treatment and control subjects differ greatly as seen in the unweighted balance comparison ($unw) from bal.table(ps.lalonde).


